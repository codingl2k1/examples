{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26921be8",
   "metadata": {},
   "source": [
    "# Get the license with most stars using Xorbits dataset over bigcode/the-stack Hugging Face dataset\n",
    "\n",
    "In this notebook, we will demonstrate how to use Xorbits to get the license with most stars over the bigcode/the-stack dataset.\n",
    "The Stack contains over 6TB of permissively-licensed source code files covering 358 programming languages. The dataset was created \n",
    "as part of the BigCode Project, an open scientific collaboration working on the responsible development of Large Language Models \n",
    "for Code (Code LLMs). This notebook focus on the Python language.\n",
    "\n",
    "## Software versions\n",
    "- Xorbits[datasets]>=0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install \"xorbits[datasets]>=0.5.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35192093",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "This step loads the Hugging Face dataset in parallel. First of all, you need to go to the dataset page https://huggingface.co/datasets/bigcode/the-stack\n",
    "to fill in your email to obtain authorization. Then get the access token in the access tokens tab of the setting page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e1c6a-16ac-48b8-834d-230783bfc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xorbits.pandas as pd\n",
    "import xorbits.datasets as xdatasets\n",
    "# bigcode/the-stack need your access key, please refer to https://huggingface.co/datasets/bigcode/the-stack\n",
    "ds = xdatasets.from_huggingface(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train\", token=\"<YOUR ACCESS TOKEN>\")\n",
    "# Use ArrowDtype to reduce memory usage.\n",
    "pdf = ds.to_dataframe(types_mapper=pd.ArrowDtype)\n",
    "# Eval the dataframe trigger xorbits execution, the download will be in parallel.\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52314107",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "We are only interested in these few columns of data, so filter columns first to reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d267ca4-c78d-420d-acf6-196b591cb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf[[\"max_stars_repo_name\", \"max_stars_repo_licenses\", \"max_stars_count\"]]\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5bbc96",
   "metadata": {},
   "source": [
    "The licenses are on the repo, so we need to dedup the data by `max_stars_repo_name`. As the data shown above, the `max_stars_repo_licenses`\n",
    "is a list of string, we need to convert the value to string for the subsequent groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdc16f-1ac6-4e3e-9642-d520ca5c703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[\"max_stars_repo_licenses\"] = pdf[\"max_stars_repo_licenses\"].map(lambda x: x[0], dtype=\"str\")\n",
    "pdf[\"max_stars_repo_name\"] = pdf[\"max_stars_repo_name\"].map(lambda x: x.split(\"/\")[-1], dtype=\"str\")\n",
    "pdf = pdf.drop_duplicates(subset=[\"max_stars_repo_name\"])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874c7d1",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's get the final result of the licenses with most stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e5e35-4160-4f11-b387-ca275010ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pdf.groupby(\"max_stars_repo_licenses\")[\"max_stars_count\"].sum()\n",
    "result.sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d0797",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, Xorbits dataset is a powerful tool for loading and analyzing large datasets. By following the steps outlined in this notebook, you can gain a better understanding of the capabilities of Xorbits, its ease-of-use, and how it can be integrated with other Python libraries to streamline your data analysis workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
